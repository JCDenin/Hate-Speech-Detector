import streamlit as st
import pandas as pd
import joblib
import re
import nltk
import numpy as np
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from googletrans import Translator
from wordcloud import WordCloud
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns

# --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è NLTK ---
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ç–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ ---
model = joblib.load('model.pkl')
vectorizer = joblib.load('vectorizer.pkl')

# --- –ó–∞–∫–æ–Ω–∏ ---
law_sets = {
    "üá∫üá∏ US Law": {
        "18 U.S. Code ¬ß 249 (Hate Crimes)": ['kill', 'lynch', 'burn', 'gas', 'slur', 'jew', 'exterminate'],
        "FCC obscenity rules": ['fuck', 'shit', 'bitch', 'nigger', 'faggot']
    },
    "üá™üá∫ EU Law": {
        "EU Code of Conduct on Hate Speech": ['hate', 'nazi', 'exterminate', 'terrorist', 'subhuman', 'invader']
    },
    "üá∫üá¶ Ukrainian Law": {
        "–ö–ö–£ —Å—Ç. 161 (\u041f–æ—Ä—É—à–µ–Ω–Ω—è —Ä—ñ–≤–Ω–æ–ø—Ä–∞–≤–Ω–æ—Å—Ç—ñ)": ['–∂–∏–¥–∏', '—Ö–∞—á—ñ', '–º–æ—Å–∫–∞–ª—ñ', '—á—É—Ä–∫–∏', '—á—É—Ä–∫–∞', '–Ω–µ–Ω–∞–≤–∏–¥–∂—É', '–≤–∏–ø–∏–ª—è—Ç–∏'],
        "–ö–ö–£ —Å—Ç. 300 (–†–æ–∑–ø–∞–ª—é–≤–∞–Ω–Ω—è –≤–æ—Ä–æ–∂–Ω–µ—á—ñ)": ['–∑–Ω–∏—â–∏—Ç–∏', '–æ—á–∏—Å—Ç–∏—Ç–∏', '–ª—ñ–∫–≤—ñ–¥—É–≤–∞—Ç–∏', '–≤–∏–ø–∞–ª–∏—Ç–∏']
    }
}

# --- –ü–µ—Ä–µ–∫–ª–∞–¥ ---
def translate_to_english(text):
    translator = Translator()
    try:
        return translator.translate(text, src='auto', dest='en').text
    except:
        return text

# --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–∞–∫–æ–Ω—ñ–≤ ---
def legal_check_multi(original, translated):
    results = []
    orig = original.lower()
    for law, keywords in law_sets["üá∫üá¶ Ukrainian Law"].items():
        if any(word in orig for word in keywords):
            results.append(f"üá∫üá¶ Ukrainian Law ‚Äì {law}")

    translated = translated.lower()
    for region in ["üá∫üá∏ US Law", "üá™üá∫ EU Law"]:
        for law, keywords in law_sets[region].items():
            if any(word in translated for word in keywords):
                results.append(f"{region} ‚Äì {law}")

    return results if results else ["No clear legal violation"]

# --- –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ ---
def preprocess(text):
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-zA-Z]", " ", text)
    text = text.lower()
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)

# --- –Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---
st.title("üí¨ Hate Speech Detector + Law Checker")
example_tweets = {
    "Hate Speech": "I hate all those people. They should all just disappear.",
    "Offensive Language": "There is nothing wrong with Ariana Grande..... Just cause she don't look like hoe.... Y'all gotta a problem.",
    "Neither": "Wishing everyone a great day and lots of sunshine!",
}
selected = st.selectbox("–û–±–µ—Ä—ñ—Ç—å –ø—Ä–∏–∫–ª–∞–¥ —Ç–≤—ñ—Ç–∞:", [""] + list(example_tweets.keys()), index=0)
if selected:
    user_input = example_tweets[selected]
    st.info(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–æ –ø—Ä–∏–∫–ª–∞–¥ ({selected}): {user_input}")
else:
    user_input = st.text_area("–í—Å—Ç–∞–≤ —Ç–≤—ñ—Ç –∞–±–æ –±—É–¥—å-—è–∫–∏–π —Ç–µ–∫—Å—Ç:")

if st.button("üîç –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏"):
    translated = translate_to_english(user_input)
    cleaned = preprocess(translated)
    vec = vectorizer.transform([cleaned])
    prediction = model.predict(vec)[0]
    label_map = {0: "Hate Speech", 1: "Offensive Language", 2: "Neither"}

    st.write("### üåê –ü–µ—Ä–µ–∫–ª–∞–¥ (–¥–ª—è –º–æ–¥–µ–ª—ñ):")
    st.code(translated)

    st.write("### üß∞ –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è:")
    st.success(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {label_map[prediction]}")

    st.write("### ‚öñÔ∏è –í—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å –∑–∞–∫–æ–Ω–æ–¥–∞–≤—Å—Ç–≤–∞–º:")
    for law in legal_check_multi(user_input, translated):
        st.warning(f"–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–µ –ø–æ—Ä—É—à–µ–Ω–Ω—è: {law}")

    st.write("### üìä –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –ø–æ –∫–ª–∞—Å–∞—Ö:")
    proba = model.predict_proba(vec)[0]
    fig, ax = plt.subplots()
    ax.bar(label_map.values(), proba, color=['red', 'orange', 'green'])
    ax.set_ylim(0, 1)
    st.pyplot(fig)

    st.write("### üå• Word Cloud:")
    wc = WordCloud(width=800, height=400, background_color='white').generate(cleaned)
    fig_wc, ax_wc = plt.subplots()
    ax_wc.imshow(wc, interpolation='bilinear')
    ax_wc.axis('off')
    st.pyplot(fig_wc)

    st.write("### üìè –î–æ–≤–∂–∏–Ω–∞ —Ç–µ–∫—Å—Ç—É:")
    length = len(user_input.split())
    fig_len, ax_len = plt.subplots()
    ax_len.bar(["Length"], [length], color='skyblue')
    ax_len.set_ylabel("–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤")
    st.pyplot(fig_len)

    st.write("### ‚ùå –ú–∞—Ç—Ä–∏—Ü—è –ø–æ–º–∏–ª–æ–∫:")
    y_true = [prediction]
    y_pred = [np.argmax(proba)]
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(label_map.values()))
    fig_cm, ax_cm = plt.subplots()
    disp.plot(ax=ax_cm, cmap='Blues')
    st.pyplot(fig_cm)
